{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "from io import open\n",
    "import glob\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Our Data ***\n",
    "We are going to be training a char level RNN on lists of baby names to generate new names in a given language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \".,;'-\"\n",
    "n_letters = len(all_letters) + 1 #end of sentence marker == \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gives you a list of files of a given type in a dir using regex\n",
    "def find_files(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "filenames = find_files(\"data/names/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII: http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Khoury',\n",
       " u'Nahas',\n",
       " u'Daher',\n",
       " u'Gerges',\n",
       " u'Nazari',\n",
       " u'Maalouf',\n",
       " u'Gerges',\n",
       " u'Naifeh',\n",
       " u'Guirguis',\n",
       " u'Baba',\n",
       " u'Sabbagh',\n",
       " u'Attia',\n",
       " u'Tahan',\n",
       " u'Haddad',\n",
       " u'Aswad',\n",
       " u'Najjar',\n",
       " u'Dagher',\n",
       " u'Maloof',\n",
       " u'Isa',\n",
       " u'Asghar',\n",
       " u'Nader',\n",
       " u'Gaber',\n",
       " u'Abboud',\n",
       " u'Maalouf',\n",
       " u'Zogby',\n",
       " u'Srour',\n",
       " u'Bahar',\n",
       " u'Mustafa',\n",
       " u'Hanania',\n",
       " u'Daher',\n",
       " u'Tuma',\n",
       " u'Nahas',\n",
       " u'Saliba',\n",
       " u'Shamoon',\n",
       " u'Handal',\n",
       " u'Baba',\n",
       " u'Amari',\n",
       " u'Bahar',\n",
       " u'Atiyeh',\n",
       " u'Said',\n",
       " u'Khouri',\n",
       " u'Tahan',\n",
       " u'Baba',\n",
       " u'Mustafa',\n",
       " u'Guirguis',\n",
       " u'Sleiman',\n",
       " u'Seif',\n",
       " u'Dagher',\n",
       " u'Bahar',\n",
       " u'Gaber',\n",
       " u'Harb',\n",
       " u'Seif',\n",
       " u'Asker',\n",
       " u'Nader',\n",
       " u'Antar',\n",
       " u'Awad',\n",
       " u'Srour',\n",
       " u'Shadid',\n",
       " u'Hajjar',\n",
       " u'Hanania',\n",
       " u'Kalb',\n",
       " u'Shadid',\n",
       " u'Bazzi',\n",
       " u'Mustafa',\n",
       " u'Masih',\n",
       " u'Ghanem',\n",
       " u'Haddad',\n",
       " u'Isa',\n",
       " u'Antoun',\n",
       " u'Sarraf',\n",
       " u'Sleiman',\n",
       " u'Dagher',\n",
       " u'Najjar',\n",
       " u'Malouf',\n",
       " u'Nahas',\n",
       " u'Naser',\n",
       " u'Saliba',\n",
       " u'Shamon',\n",
       " u'Malouf',\n",
       " u'Kalb',\n",
       " u'Daher',\n",
       " u'Maalouf',\n",
       " u'Wasem',\n",
       " u'Kanaan',\n",
       " u'Naifeh',\n",
       " u'Boutros',\n",
       " u'Moghadam',\n",
       " u'Masih',\n",
       " u'Sleiman',\n",
       " u'Aswad',\n",
       " u'Cham',\n",
       " u'Assaf',\n",
       " u'Quraishi',\n",
       " u'Shalhoub',\n",
       " u'Sabbag',\n",
       " u'Mifsud',\n",
       " u'Gaber',\n",
       " u'Shammas',\n",
       " u'Tannous',\n",
       " u'Sleiman',\n",
       " u'Bazzi',\n",
       " u'Quraishi',\n",
       " u'Rahal',\n",
       " u'Cham',\n",
       " u'Ghanem',\n",
       " u'Ghanem',\n",
       " u'Naser',\n",
       " u'Baba',\n",
       " u'Shamon',\n",
       " u'Almasi',\n",
       " u'Basara',\n",
       " u'Quraishi',\n",
       " u'Bata',\n",
       " u'Wasem',\n",
       " u'Shamoun',\n",
       " u'Deeb',\n",
       " u'Touma',\n",
       " u'Asfour',\n",
       " u'Deeb',\n",
       " u'Hadad',\n",
       " u'Naifeh',\n",
       " u'Touma',\n",
       " u'Bazzi',\n",
       " u'Shamoun',\n",
       " u'Nahas',\n",
       " u'Haddad',\n",
       " u'Arian',\n",
       " u'Kouri',\n",
       " u'Deeb',\n",
       " u'Toma',\n",
       " u'Halabi',\n",
       " u'Nazari',\n",
       " u'Saliba',\n",
       " u'Fakhoury',\n",
       " u'Hadad',\n",
       " u'Baba',\n",
       " u'Mansour',\n",
       " u'Sayegh',\n",
       " u'Antar',\n",
       " u'Deeb',\n",
       " u'Morcos',\n",
       " u'Shalhoub',\n",
       " u'Sarraf',\n",
       " u'Amari',\n",
       " u'Wasem',\n",
       " u'Ganim',\n",
       " u'Tuma',\n",
       " u'Fakhoury',\n",
       " u'Hadad',\n",
       " u'Hakimi',\n",
       " u'Nader',\n",
       " u'Said',\n",
       " u'Ganim',\n",
       " u'Daher',\n",
       " u'Ganem',\n",
       " u'Tuma',\n",
       " u'Boutros',\n",
       " u'Aswad',\n",
       " u'Sarkis',\n",
       " u'Daher',\n",
       " u'Toma',\n",
       " u'Boutros',\n",
       " u'Kanaan',\n",
       " u'Antar',\n",
       " u'Gerges',\n",
       " u'Kouri',\n",
       " u'Maroun',\n",
       " u'Wasem',\n",
       " u'Dagher',\n",
       " u'Naifeh',\n",
       " u'Bishara',\n",
       " u'Ba',\n",
       " u'Cham',\n",
       " u'Kalb',\n",
       " u'Bazzi',\n",
       " u'Bitar',\n",
       " u'Hadad',\n",
       " u'Moghadam',\n",
       " u'Sleiman',\n",
       " u'Shamoun',\n",
       " u'Antar',\n",
       " u'Atiyeh',\n",
       " u'Koury',\n",
       " u'Nahas',\n",
       " u'Kouri',\n",
       " u'Maroun',\n",
       " u'Nassar',\n",
       " u'Sayegh',\n",
       " u'Haik',\n",
       " u'Ghanem',\n",
       " u'Sayegh',\n",
       " u'Salib',\n",
       " u'Cham',\n",
       " u'Bata',\n",
       " u'Touma',\n",
       " u'Antoun',\n",
       " u'Antar',\n",
       " u'Bata',\n",
       " u'Botros',\n",
       " u'Shammas',\n",
       " u'Ganim',\n",
       " u'Sleiman',\n",
       " u'Seif',\n",
       " u'Moghadam',\n",
       " u'Ba',\n",
       " u'Tannous',\n",
       " u'Bazzi',\n",
       " u'Seif',\n",
       " u'Salib',\n",
       " u'Hadad',\n",
       " u'Quraishi',\n",
       " u'Halabi',\n",
       " u'Essa',\n",
       " u'Bahar',\n",
       " u'Kattan',\n",
       " u'Boutros',\n",
       " u'Nahas',\n",
       " u'Sabbagh',\n",
       " u'Kanaan',\n",
       " u'Sayegh',\n",
       " u'Said',\n",
       " u'Botros',\n",
       " u'Najjar',\n",
       " u'Toma',\n",
       " u'Bata',\n",
       " u'Atiyeh',\n",
       " u'Halabi',\n",
       " u'Tannous',\n",
       " u'Kouri',\n",
       " u'Shamoon',\n",
       " u'Kassis',\n",
       " u'Haddad',\n",
       " u'Tuma',\n",
       " u'Mansour',\n",
       " u'Antar',\n",
       " u'Kassis',\n",
       " u'Kalb',\n",
       " u'Basara',\n",
       " u'Rahal',\n",
       " u'Mansour',\n",
       " u'Handal',\n",
       " u'Morcos',\n",
       " u'Fakhoury',\n",
       " u'Hadad',\n",
       " u'Morcos',\n",
       " u'Kouri',\n",
       " u'Quraishi',\n",
       " u'Almasi',\n",
       " u'Awad',\n",
       " u'Naifeh',\n",
       " u'Koury',\n",
       " u'Asker',\n",
       " u'Maroun',\n",
       " u'Fakhoury',\n",
       " u'Sabbag',\n",
       " u'Sarraf',\n",
       " u'Shamon',\n",
       " u'Assaf',\n",
       " u'Boutros',\n",
       " u'Malouf',\n",
       " u'Nassar',\n",
       " u'Qureshi',\n",
       " u'Ghanem',\n",
       " u'Srour',\n",
       " u'Almasi',\n",
       " u'Qureshi',\n",
       " u'Ghannam',\n",
       " u'Mustafa',\n",
       " u'Najjar',\n",
       " u'Kassab',\n",
       " u'Shadid',\n",
       " u'Shamoon',\n",
       " u'Morcos',\n",
       " u'Atiyeh',\n",
       " u'Isa',\n",
       " u'Ba',\n",
       " u'Baz',\n",
       " u'Asker',\n",
       " u'Seif',\n",
       " u'Asghar',\n",
       " u'Hajjar',\n",
       " u'Deeb',\n",
       " u'Essa',\n",
       " u'Qureshi',\n",
       " u'Abboud',\n",
       " u'Ganem',\n",
       " u'Haddad',\n",
       " u'Koury',\n",
       " u'Nassar',\n",
       " u'Abadi',\n",
       " u'Toma',\n",
       " u'Tannous',\n",
       " u'Harb',\n",
       " u'Issa',\n",
       " u'Khouri',\n",
       " u'Mifsud',\n",
       " u'Kalb',\n",
       " u'Gaber',\n",
       " u'Ganim',\n",
       " u'Boulos',\n",
       " u'Samaha',\n",
       " u'Haddad',\n",
       " u'Sabbag',\n",
       " u'Wasem',\n",
       " u'Dagher',\n",
       " u'Rahal',\n",
       " u'Atiyeh',\n",
       " u'Antar',\n",
       " u'Asghar',\n",
       " u'Mansour',\n",
       " u'Awad',\n",
       " u'Boulos',\n",
       " u'Sarraf',\n",
       " u'Deeb',\n",
       " u'Abadi',\n",
       " u'Nazari',\n",
       " u'Daher',\n",
       " u'Gerges',\n",
       " u'Shamoon',\n",
       " u'Gaber',\n",
       " u'Amari',\n",
       " u'Sarraf',\n",
       " u'Nazari',\n",
       " u'Saliba',\n",
       " u'Naifeh',\n",
       " u'Nazari',\n",
       " u'Hakimi',\n",
       " u'Shamon',\n",
       " u'Abboud',\n",
       " u'Quraishi',\n",
       " u'Tahan',\n",
       " u'Safar',\n",
       " u'Hajjar',\n",
       " u'Srour',\n",
       " u'Gaber',\n",
       " u'Shalhoub',\n",
       " u'Attia',\n",
       " u'Safar',\n",
       " u'Said',\n",
       " u'Ganem',\n",
       " u'Nader',\n",
       " u'Asghar',\n",
       " u'Mustafa',\n",
       " u'Said',\n",
       " u'Antar',\n",
       " u'Botros',\n",
       " u'Nader',\n",
       " u'Ghannam',\n",
       " u'Asfour',\n",
       " u'Tahan',\n",
       " u'Mansour',\n",
       " u'Attia',\n",
       " u'Touma',\n",
       " u'Najjar',\n",
       " u'Kassis',\n",
       " u'Abboud',\n",
       " u'Bishara',\n",
       " u'Bazzi',\n",
       " u'Shalhoub',\n",
       " u'Shalhoub',\n",
       " u'Safar',\n",
       " u'Khoury',\n",
       " u'Nazari',\n",
       " u'Sabbag',\n",
       " u'Sleiman',\n",
       " u'Atiyeh',\n",
       " u'Kouri',\n",
       " u'Bitar',\n",
       " u'Zogby',\n",
       " u'Ghanem',\n",
       " u'Assaf',\n",
       " u'Abadi',\n",
       " u'Arian',\n",
       " u'Shalhoub',\n",
       " u'Khoury',\n",
       " u'Morcos',\n",
       " u'Shamon',\n",
       " u'Wasem',\n",
       " u'Abadi',\n",
       " u'Antoun',\n",
       " u'Baz',\n",
       " u'Naser',\n",
       " u'Assaf',\n",
       " u'Saliba',\n",
       " u'Nader',\n",
       " u'Mikhail',\n",
       " u'Naser',\n",
       " u'Daher',\n",
       " u'Morcos',\n",
       " u'Awad',\n",
       " u'Nahas',\n",
       " u'Sarkis',\n",
       " u'Malouf',\n",
       " u'Mustafa',\n",
       " u'Fakhoury',\n",
       " u'Ghannam',\n",
       " u'Shadid',\n",
       " u'Gaber',\n",
       " u'Koury',\n",
       " u'Atiyeh',\n",
       " u'Shamon',\n",
       " u'Boutros',\n",
       " u'Sarraf',\n",
       " u'Arian',\n",
       " u'Fakhoury',\n",
       " u'Abadi',\n",
       " u'Kassab',\n",
       " u'Nahas',\n",
       " u'Quraishi',\n",
       " u'Mansour',\n",
       " u'Samaha',\n",
       " u'Wasem',\n",
       " u'Seif',\n",
       " u'Fakhoury',\n",
       " u'Saliba',\n",
       " u'Cham',\n",
       " u'Bahar',\n",
       " u'Shamoun',\n",
       " u'Essa',\n",
       " u'Shamon',\n",
       " u'Asfour',\n",
       " u'Bitar',\n",
       " u'Cham',\n",
       " u'Tahan',\n",
       " u'Tannous',\n",
       " u'Daher',\n",
       " u'Khoury',\n",
       " u'Shamon',\n",
       " u'Bahar',\n",
       " u'Quraishi',\n",
       " u'Ghannam',\n",
       " u'Kassab',\n",
       " u'Zogby',\n",
       " u'Basara',\n",
       " u'Shammas',\n",
       " u'Arian',\n",
       " u'Sayegh',\n",
       " u'Naifeh',\n",
       " u'Mifsud',\n",
       " u'Sleiman',\n",
       " u'Arian',\n",
       " u'Kassis',\n",
       " u'Shamoun',\n",
       " u'Kassis',\n",
       " u'Harb',\n",
       " u'Mustafa',\n",
       " u'Boulos',\n",
       " u'Asghar',\n",
       " u'Shamon',\n",
       " u'Kanaan',\n",
       " u'Atiyeh',\n",
       " u'Kassab',\n",
       " u'Tahan',\n",
       " u'Bazzi',\n",
       " u'Kassis',\n",
       " u'Qureshi',\n",
       " u'Basara',\n",
       " u'Shalhoub',\n",
       " u'Sayegh',\n",
       " u'Haik',\n",
       " u'Attia',\n",
       " u'Maroun',\n",
       " u'Kassis',\n",
       " u'Sarkis',\n",
       " u'Harb',\n",
       " u'Assaf',\n",
       " u'Kattan',\n",
       " u'Antar',\n",
       " u'Sleiman',\n",
       " u'Touma',\n",
       " u'Sarraf',\n",
       " u'Bazzi',\n",
       " u'Boulos',\n",
       " u'Baz',\n",
       " u'Issa',\n",
       " u'Shamon',\n",
       " u'Shadid',\n",
       " u'Deeb',\n",
       " u'Sabbag',\n",
       " u'Wasem',\n",
       " u'Awad',\n",
       " u'Mansour',\n",
       " u'Saliba',\n",
       " u'Fakhoury',\n",
       " u'Arian',\n",
       " u'Bishara',\n",
       " u'Dagher',\n",
       " u'Bishara',\n",
       " u'Koury',\n",
       " u'Fakhoury',\n",
       " u'Naser',\n",
       " u'Nader',\n",
       " u'Antar',\n",
       " u'Gerges',\n",
       " u'Handal',\n",
       " u'Hanania',\n",
       " u'Shadid',\n",
       " u'Gerges',\n",
       " u'Kassis',\n",
       " u'Essa',\n",
       " u'Assaf',\n",
       " u'Shadid',\n",
       " u'Seif',\n",
       " u'Shalhoub',\n",
       " u'Shamoun',\n",
       " u'Hajjar',\n",
       " u'Baba',\n",
       " u'Sayegh',\n",
       " u'Mustafa',\n",
       " u'Sabbagh',\n",
       " u'Isa',\n",
       " u'Najjar',\n",
       " u'Tannous',\n",
       " u'Hanania',\n",
       " u'Ganem',\n",
       " u'Gerges',\n",
       " u'Fakhoury',\n",
       " u'Mifsud',\n",
       " u'Nahas',\n",
       " u'Bishara',\n",
       " u'Bishara',\n",
       " u'Abadi',\n",
       " u'Sarkis',\n",
       " u'Masih',\n",
       " u'Isa',\n",
       " u'Attia',\n",
       " u'Kalb',\n",
       " u'Essa',\n",
       " u'Boulos',\n",
       " u'Basara',\n",
       " u'Halabi',\n",
       " u'Halabi',\n",
       " u'Dagher',\n",
       " u'Attia',\n",
       " u'Kassis',\n",
       " u'Tuma',\n",
       " u'Gerges',\n",
       " u'Ghannam',\n",
       " u'Toma',\n",
       " u'Baz',\n",
       " u'Asghar',\n",
       " u'Zogby',\n",
       " u'Aswad',\n",
       " u'Hadad',\n",
       " u'Dagher',\n",
       " u'Naser',\n",
       " u'Shadid',\n",
       " u'Atiyeh',\n",
       " u'Zogby',\n",
       " u'Abboud',\n",
       " u'Tannous',\n",
       " u'Khouri',\n",
       " u'Atiyeh',\n",
       " u'Ganem',\n",
       " u'Maalouf',\n",
       " u'Isa',\n",
       " u'Maroun',\n",
       " u'Issa',\n",
       " u'Khouri',\n",
       " u'Harb',\n",
       " u'Nader',\n",
       " u'Awad',\n",
       " u'Nahas',\n",
       " u'Said',\n",
       " u'Baba',\n",
       " u'Totah',\n",
       " u'Ganim',\n",
       " u'Handal',\n",
       " u'Mansour',\n",
       " u'Basara',\n",
       " u'Malouf',\n",
       " u'Said',\n",
       " u'Botros',\n",
       " u'Samaha',\n",
       " u'Safar',\n",
       " u'Tahan',\n",
       " u'Botros',\n",
       " u'Shamoun',\n",
       " u'Handal',\n",
       " u'Sarraf',\n",
       " u'Malouf',\n",
       " u'Bishara',\n",
       " u'Aswad',\n",
       " u'Khouri',\n",
       " u'Baz',\n",
       " u'Asker',\n",
       " u'Toma',\n",
       " u'Koury',\n",
       " u'Gerges',\n",
       " u'Bishara',\n",
       " u'Boulos',\n",
       " u'Najjar',\n",
       " u'Aswad',\n",
       " u'Shamon',\n",
       " u'Kouri',\n",
       " u'Srour',\n",
       " u'Assaf',\n",
       " u'Tannous',\n",
       " u'Attia',\n",
       " u'Mustafa',\n",
       " u'Kattan',\n",
       " u'Asghar',\n",
       " u'Amari',\n",
       " u'Shadid',\n",
       " u'Said',\n",
       " u'Bazzi',\n",
       " u'Masih',\n",
       " u'Antar',\n",
       " u'Fakhoury',\n",
       " u'Shadid',\n",
       " u'Masih',\n",
       " u'Handal',\n",
       " u'Sarraf',\n",
       " u'Kassis',\n",
       " u'Salib',\n",
       " u'Hajjar',\n",
       " u'Totah',\n",
       " u'Koury',\n",
       " u'Totah',\n",
       " u'Mustafa',\n",
       " u'Sabbagh',\n",
       " u'Moghadam',\n",
       " u'Toma',\n",
       " u'Srour',\n",
       " u'Almasi',\n",
       " u'Totah',\n",
       " u'Maroun',\n",
       " u'Kattan',\n",
       " u'Naifeh',\n",
       " u'Sarkis',\n",
       " u'Mikhail',\n",
       " u'Nazari',\n",
       " u'Boutros',\n",
       " u'Guirguis',\n",
       " u'Gaber',\n",
       " u'Kassis',\n",
       " u'Masih',\n",
       " u'Hanania',\n",
       " u'Maloof',\n",
       " u'Quraishi',\n",
       " u'Cham',\n",
       " u'Hadad',\n",
       " u'Tahan',\n",
       " u'Bitar',\n",
       " u'Arian',\n",
       " u'Gaber',\n",
       " u'Baz',\n",
       " u'Mansour',\n",
       " u'Kalb',\n",
       " u'Sarkis',\n",
       " u'Attia',\n",
       " u'Antar',\n",
       " u'Asfour',\n",
       " u'Said',\n",
       " u'Essa',\n",
       " u'Koury',\n",
       " u'Hadad',\n",
       " u'Tuma',\n",
       " u'Moghadam',\n",
       " u'Sabbagh',\n",
       " u'Amari',\n",
       " u'Dagher',\n",
       " u'Srour',\n",
       " u'Antoun',\n",
       " u'Sleiman',\n",
       " u'Maroun',\n",
       " u'Tuma',\n",
       " u'Nahas',\n",
       " u'Hanania',\n",
       " u'Sayegh',\n",
       " u'Amari',\n",
       " u'Sabbagh',\n",
       " u'Said',\n",
       " u'Cham',\n",
       " u'Asker',\n",
       " u'Nassar',\n",
       " u'Bitar',\n",
       " u'Said',\n",
       " u'Dagher',\n",
       " u'Safar',\n",
       " u'Khouri',\n",
       " u'Totah',\n",
       " u'Khoury',\n",
       " u'Salib',\n",
       " u'Basara',\n",
       " u'Abboud',\n",
       " u'Baz',\n",
       " u'Isa',\n",
       " u'Cham',\n",
       " u'Amari',\n",
       " u'Mifsud',\n",
       " u'Hadad',\n",
       " u'Rahal',\n",
       " u'Khoury',\n",
       " u'Bazzi',\n",
       " u'Basara',\n",
       " u'Totah',\n",
       " u'Ghannam',\n",
       " u'Koury',\n",
       " u'Malouf',\n",
       " u'Zogby',\n",
       " u'Zogby',\n",
       " u'Boutros',\n",
       " u'Nassar',\n",
       " u'Handal',\n",
       " u'Hajjar',\n",
       " u'Maloof',\n",
       " u'Abadi',\n",
       " u'Maroun',\n",
       " u'Mifsud',\n",
       " u'Kalb',\n",
       " u'Amari',\n",
       " u'Hakimi',\n",
       " u'Boutros',\n",
       " u'Masih',\n",
       " u'Kattan',\n",
       " u'Haddad',\n",
       " u'Arian',\n",
       " u'Nazari',\n",
       " u'Assaf',\n",
       " u'Attia',\n",
       " u'Wasem',\n",
       " u'Gerges',\n",
       " u'Asker',\n",
       " u'Tahan',\n",
       " u'Fakhoury',\n",
       " u'Shadid',\n",
       " u'Sarraf',\n",
       " u'Attia',\n",
       " u'Naifeh',\n",
       " u'Aswad',\n",
       " u'Deeb',\n",
       " u'Tannous',\n",
       " u'Totah',\n",
       " u'Cham',\n",
       " u'Baba',\n",
       " u'Najjar',\n",
       " u'Hajjar',\n",
       " u'Shamoon',\n",
       " u'Handal',\n",
       " u'Awad',\n",
       " u'Guirguis',\n",
       " u'Awad',\n",
       " u'Ganem',\n",
       " u'Naifeh',\n",
       " u'Khoury',\n",
       " u'Hajjar',\n",
       " u'Moghadam',\n",
       " u'Mikhail',\n",
       " u'Ghannam',\n",
       " u'Guirguis',\n",
       " u'Tannous',\n",
       " u'Kanaan',\n",
       " u'Handal',\n",
       " u'Khoury',\n",
       " u'Kalb',\n",
       " u'Qureshi',\n",
       " u'Najjar',\n",
       " u'Atiyeh',\n",
       " u'Gerges',\n",
       " u'Nassar',\n",
       " u'Tahan',\n",
       " u'Hadad',\n",
       " u'Fakhoury',\n",
       " u'Salib',\n",
       " u'Wasem',\n",
       " u'Bitar',\n",
       " u'Fakhoury',\n",
       " u'Attia',\n",
       " u'Awad',\n",
       " u'Totah',\n",
       " u'Deeb',\n",
       " u'Touma',\n",
       " u'Botros',\n",
       " u'Nazari',\n",
       " u'Nahas',\n",
       " u'Kouri',\n",
       " u'Ghannam',\n",
       " u'Assaf',\n",
       " u'Asfour',\n",
       " u'Sarraf',\n",
       " u'Naifeh',\n",
       " u'Toma',\n",
       " u'Asghar',\n",
       " u'Abboud',\n",
       " u'Issa',\n",
       " u'Sabbag',\n",
       " u'Sabbagh',\n",
       " u'Isa',\n",
       " u'Koury',\n",
       " u'Kattan',\n",
       " u'Shamoon',\n",
       " u'Rahal',\n",
       " u'Kalb',\n",
       " u'Naser',\n",
       " u'Masih',\n",
       " u'Sayegh',\n",
       " u'Dagher',\n",
       " u'Asker',\n",
       " u'Maroun',\n",
       " u'Dagher',\n",
       " u'Sleiman',\n",
       " u'Botros',\n",
       " u'Sleiman',\n",
       " u'Harb',\n",
       " u'Tahan',\n",
       " u'Tuma',\n",
       " u'Said',\n",
       " u'Hadad',\n",
       " u'Samaha',\n",
       " u'Harb',\n",
       " u'Cham',\n",
       " u'Atiyeh',\n",
       " u'Haik',\n",
       " u'Malouf',\n",
       " u'Bazzi',\n",
       " u'Harb',\n",
       " u'Malouf',\n",
       " u'Ghanem',\n",
       " u'Cham',\n",
       " u'Asghar',\n",
       " u'Samaha',\n",
       " u'Khouri',\n",
       " u'Nassar',\n",
       " u'Rahal',\n",
       " u'Baz',\n",
       " u'Kalb',\n",
       " u'Rahal',\n",
       " u'Gerges',\n",
       " u'Cham',\n",
       " u'Sayegh',\n",
       " u'Shadid',\n",
       " u'Morcos',\n",
       " u'Shamoon',\n",
       " u'Hakimi',\n",
       " u'Shamoon',\n",
       " u'Qureshi',\n",
       " u'Ganim',\n",
       " u'Shadid',\n",
       " u'Khoury',\n",
       " u'Boutros',\n",
       " u'Hanania',\n",
       " u'Antoun',\n",
       " u'Naifeh',\n",
       " u'Deeb',\n",
       " u'Samaha',\n",
       " u'Awad',\n",
       " u'Asghar',\n",
       " u'Awad',\n",
       " u'Saliba',\n",
       " u'Shamoun',\n",
       " u'Mikhail',\n",
       " u'Hakimi',\n",
       " u'Mikhail',\n",
       " u'Cham',\n",
       " u'Halabi',\n",
       " u'Sarkis',\n",
       " u'Kattan',\n",
       " u'Nazari',\n",
       " u'Safar',\n",
       " u'Morcos',\n",
       " u'Khoury',\n",
       " u'Essa',\n",
       " u'Nassar',\n",
       " u'Haik',\n",
       " u'Shadid',\n",
       " u'Fakhoury',\n",
       " u'Najjar',\n",
       " u'Arian',\n",
       " u'Botros',\n",
       " u'Daher',\n",
       " u'Saliba',\n",
       " u'Saliba',\n",
       " u'Kattan',\n",
       " u'Hajjar',\n",
       " u'Nader',\n",
       " u'Daher',\n",
       " u'Nassar',\n",
       " u'Maroun',\n",
       " u'Harb',\n",
       " u'Nassar',\n",
       " u'Antar',\n",
       " u'Shammas',\n",
       " u'Toma',\n",
       " u'Antar',\n",
       " u'Koury',\n",
       " u'Nader',\n",
       " u'Botros',\n",
       " u'Bahar',\n",
       " u'Najjar',\n",
       " u'Maloof',\n",
       " u'Salib',\n",
       " u'Malouf',\n",
       " u'Mansour',\n",
       " u'Bazzi',\n",
       " u'Atiyeh',\n",
       " u'Kanaan',\n",
       " u'Bishara',\n",
       " u'Hakimi',\n",
       " u'Saliba',\n",
       " u'Tuma',\n",
       " u'Mifsud',\n",
       " u'Hakimi',\n",
       " u'Assaf',\n",
       " u'Nassar',\n",
       " u'Sarkis',\n",
       " u'Bitar',\n",
       " u'Isa',\n",
       " u'Halabi',\n",
       " u'Shamon',\n",
       " u'Qureshi',\n",
       " u'Bishara',\n",
       " u'Maalouf',\n",
       " u'Srour',\n",
       " u'Boulos',\n",
       " u'Safar',\n",
       " u'Shamoun',\n",
       " u'Ganim',\n",
       " u'Abadi',\n",
       " u'Koury',\n",
       " u'Shadid',\n",
       " u'Zogby',\n",
       " u'Boutros',\n",
       " u'Shadid',\n",
       " u'Hakimi',\n",
       " u'Bazzi',\n",
       " u'Isa',\n",
       " u'Totah',\n",
       " u'Salib',\n",
       " u'Shamoon',\n",
       " u'Gaber',\n",
       " u'Antar',\n",
       " u'Antar',\n",
       " u'Najjar',\n",
       " u'Fakhoury',\n",
       " u'Malouf',\n",
       " u'Salib',\n",
       " u'Rahal',\n",
       " u'Boulos',\n",
       " u'Attia',\n",
       " u'Said',\n",
       " u'Kassis',\n",
       " u'Bahar',\n",
       " u'Bazzi',\n",
       " u'Srour',\n",
       " u'Antar',\n",
       " u'Nahas',\n",
       " u'Kassis',\n",
       " u'Samaha',\n",
       " u'Quraishi',\n",
       " u'Asghar',\n",
       " u'Asker',\n",
       " u'Antar',\n",
       " u'Totah',\n",
       " u'Haddad',\n",
       " u'Maloof',\n",
       " u'Kouri',\n",
       " u'Basara',\n",
       " u'Bata',\n",
       " u'Antar',\n",
       " u'Shammas',\n",
       " u'Arian',\n",
       " u'Gerges',\n",
       " u'Seif',\n",
       " u'Almasi',\n",
       " u'Tuma',\n",
       " u'Shamoon',\n",
       " u'Khoury',\n",
       " u'Hakimi',\n",
       " u'Abboud',\n",
       " u'Baz',\n",
       " u'Seif',\n",
       " u'Issa',\n",
       " u'Nazari',\n",
       " u'Harb',\n",
       " u'Shammas',\n",
       " u'Amari',\n",
       " u'Totah',\n",
       " u'Malouf',\n",
       " u'Sarkis',\n",
       " u'Naser',\n",
       " u'Zogby',\n",
       " u'Handal',\n",
       " u'Naifeh',\n",
       " u'Cham',\n",
       " u'Hadad',\n",
       " u'Gerges',\n",
       " u'Kalb',\n",
       " u'Shalhoub',\n",
       " u'Saliba',\n",
       " u'Tannous',\n",
       " u'Tahan',\n",
       " u'Tannous',\n",
       " u'Kassis',\n",
       " u'Shadid',\n",
       " u'Sabbag',\n",
       " u'Tahan',\n",
       " u'Abboud',\n",
       " u'Nahas',\n",
       " u'Shamoun',\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all lines of a file as a list of ascii_strings\n",
    "def read_lines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "read_lines(filenames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building categories so that we can train each language seperatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Abl'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category_lines dictionary\n",
    "# key: language\n",
    "# value: list of names in that language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "for filename in find_files('data/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = read_lines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "all_categories[0]\n",
    "category_lines['Czech'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Data Representation ***\n",
    "We need some representation of charecters that a network can understand as well as one that it can compare error in between.\n",
    "\n",
    "How do you know how to update things when you are wrong? And if you are wrong, what is the differernce between precicting q vs. s when you should have predicited t\n",
    "\n",
    "To do this we will use a \"one-hot-encoding\" of letters. A one-hot vector is a 1 x n_letters vector is filled with 0s except for a 1 at index of the represented value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1     0     0     0     0     0     0     0     0     0\n",
       "    0     1     0     0     0     0     0     0     0     0\n",
       "    0     0     1     0     0     0     0     0     0     0\n",
       "    0     0     0     1     0     0     0     0     0     0\n",
       "    0     0     0     0     1     0     0     0     0     0\n",
       "    0     0     0     0     0     1     0     0     0     0\n",
       "    0     0     0     0     0     0     1     0     0     0\n",
       "    0     0     0     0     0     0     0     0     1     0\n",
       "    0     0     0     0     0     0     0     0     0     1\n",
       "    0     0     0     0     0     0     0     0     0     0\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_index(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "def letter_to_vec(letter):\n",
    "    vec = torch.zeros(1, n_letters)\n",
    "    vec[0][get_index(letter)] = 1\n",
    "    return vec\n",
    "\n",
    "def sen_to_tensor(line):\n",
    "    #tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    tensor = torch.zeros(n_letters, 1, len(line))\n",
    "    for pos, letter in enumerate(line):\n",
    "        tensor[pos][0][get_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "sen_to_tensor(\"abcdefg hijk\")[:10,0,:10].transpose(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** So what are we doing anyway? ***\n",
    "\n",
    "char-rnn's: hour goal here is to generate text through a char-rnn but what does that really mean? \n",
    "\n",
    "On a very high level we're just going to throw a huge mass of text (represetnted as vectors at each step) at a model and then ask it to predicit the probablity distribution of the next character in given the characters its predicted so far. This will let us prectict words one char at a time.\n",
    "\n",
    "So where do we start?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Arch\n",
    "\n",
    "Well, lets \"mathamatically\" define the inputs and outputs.\n",
    "\n",
    "## input\n",
    "To predict the next character, conditional on the language we are in and based ont he characters we've predicted so far, our true input needs three things:\n",
    "\n",
    "Category: the language we are generating in (1-hot vector)\n",
    "\n",
    "Input: the last char we predicited\n",
    "\n",
    "Hidden: a vector that is some latent representation of our current state\n",
    "\n",
    "## hidden layer(s)\n",
    "\n",
    "From the input layers we apply essentially just a two independent linear layers on the input vector.\n",
    "\n",
    "One to determing the probablities of the next char for the output, and a seperate to update the state.\n",
    "\n",
    "## output layer\n",
    "We next combine the hidden state and the probablities of the next letter into a single vector, run that through another linear layer, apply dropout to regularize / prevent over-fitting, and then finally, use a softmax layer to pick the most likely next letter from the predicted probalities.  \n",
    "\n",
    "Here is the arch visually:\n",
    "\n",
    "![alt text][logo]\n",
    "\n",
    "[logo]: network_arch.png\n",
    "\n",
    "### lets see what that looks like in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # the true input len = num_categories + len(1-hot of next char) + len(hidden state representation)\n",
    "        self.input_layer_size = n_categories + input_size + hidden_size\n",
    "\n",
    "        # input_size -> hidden_size\n",
    "        self.i2h = nn.Linear(self.input_layer_size, hidden_size)\n",
    "\n",
    "        # input_size -> output_size\n",
    "        self.i2o = nn.Linear(self.input_layer_size, output_size)\n",
    "\n",
    "        # hidden_size + output_size -> output_size\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, category, input_vec, hidden_vec):\n",
    "        \n",
    "        # combine inputs by concatonating by row\n",
    "        input_combined = torch.cat((category, input_vec, hidden_vec), 1)\n",
    "        \n",
    "        hidden_vec = self.i2h(input_combined)\n",
    "        output_vec = self.i2o(input_combined)\n",
    "        \n",
    "        output_combined = torch.cat((hidden_vec, output_vec), 1)\n",
    "        \n",
    "        output_vec = self.o2o(output_combined)\n",
    "        output_vec = self.dropout(output_vec)\n",
    "        output_vec = self.softmax(output_vec)\n",
    "        \n",
    "        return output_vec, hidden_vec\n",
    "    \n",
    "    # when we initalize the network - the hidden state starts off blank\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Model Assumtions\n",
    "\n",
    "A common assumtion in many machine learning models is that the training data is that the data is drawn iid from the population.\n",
    "\n",
    "This is SELDOM true. For example, take a look at the first 10 french names we are training our name generator on:\n",
    "\n",
    "1.  Abel\n",
    "2.  Abraham\n",
    "3.  Adam\n",
    "4.  Albert\n",
    "5.  Allard\n",
    "6.  Archambault\n",
    "7.  Armistead\n",
    "8.  Arthur\n",
    "9.  Augustin\n",
    "10. Babineaux\n",
    "\n",
    "Notice something? This isnt a unique situation to our problem => data is many times stored in a format that maintatins some sort of relationship between adjcent data points. This is exactly what we dont want however. Therefore, we will write us some data loading functinos that will help us get a random line from a random category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# pick random item from list l\n",
    "def random_elem(l):\n",
    "    return l[random.randint(0,len(l) -1)]\n",
    "\n",
    "def random_cat_and_line():\n",
    "    cat = random_elem(all_categories)\n",
    "    line = random_elem(category_lines[cat])\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converting to tensors\n",
    "Recall that the network operates on tensors - not charectars. Therefore we need to convert the category into its one-hot representation.\n",
    "\n",
    "We will also the line (name) into a matrix where each element of the matrix is the one hot vector of the coresponding letter in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  5\n",
       "  6\n",
       " -1\n",
       "  7\n",
       "  8\n",
       "  9\n",
       " 10\n",
       " 57\n",
       "[torch.LongTensor of size 12]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_category_tensor(category):\n",
    "    i = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][i] = 1\n",
    "    return tensor\n",
    "\n",
    "def sample_tensor(word):\n",
    "    tensor = torch.zeros(len(word), 1, n_letters)\n",
    "    for i in range(len(word)):\n",
    "        letter = word[i]\n",
    "        #### COME BACK TO THIS LINE TO CHECK ITT #######3\n",
    "        tensor[i][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def get_target_tensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "get_target_tensor(\"abcdefg hijk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tying it all together, this funtion\n",
    "# picks a random word from a random category\n",
    "# converts the word and its category to their vector representations\n",
    "# generates the label to comapre the generated word to at each step\n",
    "def random_train_example():\n",
    "    category, word = random_cat_and_line()\n",
    "    category_tensor = Variable(get_category_tensor(category))\n",
    "    name_tensor = Variable(sample_tensor(word))\n",
    "    name_target_tensor = Variable(get_target_tensor(word))\n",
    "    return category_tensor, name_tensor, name_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using pytorch dataset / dataloader instead\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        # init all_categories and category_lines\n",
    "        self.all_categories = []\n",
    "        self.category_lines = {}\n",
    "        \n",
    "        for filename in find_files(path):\n",
    "            category = filename.split('/')[-1].split('.')[0]\n",
    "            self.all_categories.append(category)\n",
    "            lines = read_lines(filename)\n",
    "            self.category_lines[category] = lines\n",
    "        \n",
    "        self.n_categories = len(self.all_categories)\n",
    "        self.n_names_per_category = [len(self.category_lines[category]) for category in self.all_categories]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(self.n_names_per_category)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        category_index = 0\n",
    "        name_index = index\n",
    "        \n",
    "        # find which category the name belongs in, as well as the index of that name within that category\n",
    "        while name_index >= self.n_names_per_category[category_index]:\n",
    "            name_index -= self.n_names_per_category[category_index]\n",
    "            category_index += 1\n",
    "        \n",
    "        category = self.all_categories[category_index]\n",
    "        word = category_lines[category][name_index]\n",
    "        \n",
    "        # convert to tensors\n",
    "        category_tensor = get_category_tensor(category)\n",
    "        name_tensor = sample_tensor(word)\n",
    "        name_target_tensor = get_target_tensor(word)\n",
    "        \n",
    "        return category_tensor, name_tensor, name_target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I'm getting tired of writting explinations so if the quality of this degrades as we move forward blame it on 233 being a thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "So we have a recurrent network and a way to load examples into that network, how do we train it?\n",
    "\n",
    "Pulling from karpathy's blog recall what we're trying to do. As a working example, suppose we only had a vocabulary of four possible letters “helo”, and wanted to train the RNN on the training sequence “hello”. \n",
    "\n",
    "This training sequence actually ends up feeding 4 separate training examples: \n",
    "\n",
    "1. The probability of “e” should be likely given the context of “h”, \n",
    "2. “l” should be likely in the context of “he”, \n",
    "3. “l” should also be likely given the context of “hel”, and finally \n",
    "4. “o” should be likely given the context of “hell”.\n",
    "\n",
    "\n",
    "The high level version of training is to, at every timestep ask the network to predict what it expects the next letter to be given the context so far. Then if it mis-predicts we can propegate the error between what it predicts and what it should have predicted back through the network using back-prop.\n",
    "\n",
    "Through back-prop we can figure out in what direction we should adjust each weight in the network to increase the scores of the correct targets. Using that we perform a parameter update, which nudges every weight a tiny amount in this gradient direction. \n",
    "\n",
    "***Note: for a great explination of backprop check out:*** http://neuralnetworksanddeeplearning.com/chap2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "def train(category_tensor, sample_tensor, target_tensor):\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(sample_tensor.size()[0]):\n",
    "        output, hidden = rnn(category_tensor, sample_tensor[i], hidden)\n",
    "        loss += criterion(output, target_tensor[i])\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "        \n",
    "    return output, loss.data[0] / target_tensor.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 17s (5000 5%) 3.3682\n",
      "2m 32s (10000 10%) 2.1872\n",
      "3m 46s (15000 15%) 2.8471\n",
      "4m 57s (20000 20%) 2.0927\n",
      "6m 3s (25000 25%) 2.7306\n",
      "7m 11s (30000 30%) 2.3500\n",
      "8m 18s (35000 35%) 2.0308\n",
      "9m 26s (40000 40%) 2.6501\n",
      "10m 36s (45000 45%) 2.5331\n",
      "11m 45s (50000 50%) 3.7872\n",
      "12m 54s (55000 55%) 2.0677\n",
      "14m 6s (60000 60%) 2.1007\n",
      "15m 18s (65000 65%) 1.6486\n",
      "16m 21s (70000 70%) 2.6857\n",
      "17m 32s (75000 75%) 3.7048\n",
      "18m 42s (80000 80%) 1.6884\n",
      "19m 54s (85000 85%) 2.2310\n",
      "21m 4s (90000 90%) 1.6593\n",
      "22m 8s (95000 95%) 1.8860\n",
      "23m 11s (100000 100%) 2.1244\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "\n",
    "names_dataset = NamesDataset(path=\"data/names/*.txt\")\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0\n",
    "\n",
    "# NOTE: CANNOT HAVE A BATCH SIZE > 1 BECAUSE WORDS ARE OF DIFFERENT LENGTH, THEREFORE\n",
    "# TENSORS WILL ALSO BE OF DIFFERENT DIMENSIONS AND CAN'T BE COMBINED\n",
    "batch_size = 1\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "iter = 0\n",
    "while iter < n_iters:\n",
    "    # will this crash / cause errors with batch size > 1 and workers > 1 once this gets hit for the second time?\n",
    "    # don't know because can't test batching\n",
    "    dataloader = DataLoader(names_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    for i_batch, training_example in enumerate(dataloader):\n",
    "        iter += 1\n",
    "        \n",
    "        if iter > n_iters:\n",
    "            break\n",
    "            \n",
    "        category_tensor, name_tensor, name_target_tensor = training_example\n",
    "        \n",
    "        # dataloader adds an extra dimension because of batches\n",
    "        for i in range(batch_size):\n",
    "            output, loss = train(Variable(category_tensor[i]), Variable(name_tensor[i]), Variable(name_target_tensor[i]))\n",
    "            total_loss += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(total_loss / plot_every)\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(category, start_letter='A'):\n",
    "    category_tensor = Variable(get_category_tensor(category))\n",
    "    input = Variable(sample_tensor(start_letter))\n",
    "    hidden = rnn.init_hidden()\n",
    "\n",
    "    output_name = start_letter\n",
    "\n",
    "    for i in range(max_length):\n",
    "        output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "        topv, topi = output.data.topk(1)\n",
    "        topi = topi[0][0]\n",
    "        if topi == n_letters - 1:\n",
    "            break\n",
    "        else:\n",
    "            letter = all_letters[topi]\n",
    "            output_name += letter\n",
    "        input = Variable(sample_tensor(letter))\n",
    "\n",
    "    return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))\n",
    "\n",
    "samples('Russian', 'RUS')\n",
    "\n",
    "samples('German', 'GER')\n",
    "\n",
    "samples('Spanish', 'SPA')\n",
    "\n",
    "samples('English', 'ENG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
